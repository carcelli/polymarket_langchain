"""
GitHub ML Agent - Generates ML-based betting strategy tests

Specialized subagent that creates machine learning models and tests
for finding profitable betting opportunities, then commits them to GitHub.
"""

import os
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

from .github_agent import create_github_subagent, check_github_repo_status
from ..ml_strategies import MarketPredictor, EdgeDetector
from market_analysis_workflow import MarketAnalyzer


class MLTestGenerator:
    """Generates ML-based betting strategy tests."""

    def __init__(self):
        self.analyzer = MarketAnalyzer()
        self.base_dir = Path("./tests/ml_strategies")

    def generate_strategy_test(
        self, strategy_name: str, strategy_class: str, description: str
    ) -> str:
        """Generate a pytest file for a betting strategy."""

        # Create test file content
        test_content = f'''"""
Machine Learning Betting Strategy Tests: {strategy_name}

{description}

Generated by GitHub ML Agent on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

import pytest
import numpy as np
import pandas as pd
from typing import Dict, List, Any

from polymarket_agents.ml_strategies.{strategy_class.lower()} import {strategy_class}
from market_analysis_workflow import MarketAnalyzer


class Test{strategy_class}:
    """Test suite for {strategy_name} betting strategy."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return {strategy_class}()

    @pytest.fixture
    def sample_markets(self):
        """Get sample market data for testing."""
        analyzer = MarketAnalyzer()
        markets = analyzer.get_high_volume_markets(limit=10)
        return markets

    @pytest.fixture
    def mock_training_data(self):
        """Create mock training data."""
        np.random.seed(42)
        n_samples = 100

        data = []
        for i in range(n_samples):
            # Generate realistic market data
            volume = np.random.exponential(500000) + 10000
            yes_price = np.random.beta(2, 2)  # Concentrated around 0.5
            category = np.random.choice(['politics', 'sports', 'crypto', 'geopolitics'])

            # Simulate actual outcome (with some correlation to price)
            true_prob = yes_price + np.random.normal(0, 0.1)
            true_prob = np.clip(true_prob, 0.01, 0.99)
            actual_outcome = 1 if np.random.random() < true_prob else 0

            data.append({{
                'market_id': f'market_{i}',
                'question': f'Sample market question {i}',
                'category': category,
                'volume': volume,
                'outcome_prices': [str(yes_price), str(1-yes_price)],
                'actual_outcome': actual_outcome
            }})

        return pd.DataFrame(data)

    def test_strategy_initialization(self, strategy):
        """Test strategy can be initialized."""
        assert strategy.name == "{strategy_name}"
        assert hasattr(strategy, 'train')
        assert hasattr(strategy, 'predict')

    def test_feature_preparation(self, strategy, sample_markets):
        """Test feature preparation from market data."""
        if not sample_markets:
            pytest.skip("No sample markets available")

        market = sample_markets[0]
        features = strategy.prepare_features(market)

        # Should return a 2D array
        assert len(features.shape) == 2
        assert features.shape[0] == 1  # One sample
        assert features.shape[1] > 0  # Some features

        # Should have feature columns defined
        assert hasattr(strategy, 'feature_columns')
        assert len(strategy.feature_columns) == features.shape[1]

    def test_training_process(self, strategy, mock_training_data):
        """Test strategy training process."""
        # This would normally take time, so we'll test the interface
        assert hasattr(strategy, 'train')

        # For now, just test that the method exists and can be called
        # In a real test, you'd call strategy.train(mock_training_data)
        # and verify the model is trained

    def test_prediction_interface(self, strategy, sample_markets):
        """Test prediction interface."""
        if not sample_markets:
            pytest.skip("No sample markets available")

        market = sample_markets[0]

        # Should be able to make predictions even without training
        result = strategy.predict(market)

        # Check result structure
        assert hasattr(result, 'market_id')
        assert hasattr(result, 'predicted_probability')
        assert hasattr(result, 'confidence')
        assert hasattr(result, 'recommended_bet')
        assert hasattr(result, 'edge')
        assert hasattr(result, 'position_size')
        assert hasattr(result, 'reasoning')

        # Basic validation
        assert 0 <= result.predicted_probability <= 1
        assert 0 <= result.confidence <= 1
        assert result.recommended_bet in ['YES', 'NO', 'PASS']
        assert isinstance(result.reasoning, str)

    def test_edge_calculation(self, strategy):
        """Test edge calculation logic."""
        # Test various scenarios
        test_cases = [
            (0.6, 0.5, 0.02),  # Positive edge
            (0.4, 0.5, 0.02),  # Negative edge
            (0.5, 0.5, 0.02),  # No edge
        ]

        for pred_prob, market_prob, commission in test_cases:
            edge = strategy.calculate_edge(pred_prob, market_prob, commission)
            assert isinstance(edge, float)

    def test_kelly_criterion(self, strategy):
        """Test Kelly criterion position sizing."""
        # Positive edge
        position = strategy.kelly_criterion(0.1, 0.8)
        assert 0 <= position <= 0.25  # Max 25% limit

        # No edge
        position = strategy.kelly_criterion(0, 0.8)
        assert position == 0

        # Negative edge
        position = strategy.kelly_criterion(-0.1, 0.8)
        assert position == 0

    def test_performance_evaluation(self, strategy):
        """Test performance evaluation interface."""
        # Create mock predictions
        mock_predictions = [
            type('MockResult', (), {{
                'market_id': f'market_{i}',
                'edge': np.random.normal(0.02, 0.05),
                'confidence': np.random.uniform(0.5, 0.9),
                'expected_value': np.random.normal(0.01, 0.02)
            }})() for i in range(10)
        ]

        # Test evaluation
        performance = strategy.evaluate_performance(mock_predictions)

        assert 'total_predictions' in performance
        assert 'average_edge' in performance
        assert 'average_confidence' in performance
        assert performance['total_predictions'] == len(mock_predictions)

    @pytest.mark.parametrize("market_category", ["politics", "sports", "crypto"])
    def test_category_handling(self, strategy, market_category):
        """Test strategy handles different market categories."""
        market_data = {{
            'id': 'test_market',
            'question': f'Will something happen in {market_category}?',
            'category': market_category,
            'volume': 500000,
            'outcome_prices': ['0.55', '0.45'],
            'liquidity': 25000
        }}

        result = strategy.predict(market_data)

        # Should handle all categories gracefully
        assert result.recommended_bet in ['YES', 'NO', 'PASS']
        assert market_category.lower() in result.reasoning.lower()

    def test_extreme_market_conditions(self, strategy):
        """Test strategy behavior with extreme market conditions."""
        test_cases = [
            # Very low probability, high volume
            {{
                'id': 'extreme_1',
                'question': 'Extreme low probability event?',
                'category': 'politics',
                'volume': 2000000,
                'outcome_prices': ['0.05', '0.95'],
                'liquidity': 50000
            }},
            # Very high probability, low volume
            {{
                'id': 'extreme_2',
                'question': 'Extreme high probability event?',
                'category': 'sports',
                'volume': 50000,
                'outcome_prices': ['0.95', '0.05'],
                'liquidity': 5000
            }},
            # Exactly 50/50
            {{
                'id': 'extreme_3',
                'question': 'Perfect 50/50 event?',
                'category': 'crypto',
                'volume': 100000,
                'outcome_prices': ['0.5', '0.5'],
                'liquidity': 10000
            }}
        ]

        for market_data in test_cases:
            result = strategy.predict(market_data)

            # Should handle extreme cases gracefully
            assert result.recommended_bet in ['YES', 'NO', 'PASS']
            assert 0 <= result.predicted_probability <= 1
            assert 0 <= result.confidence <= 1

    def test_feature_importance(self, strategy):
        """Test feature importance reporting."""
        importance = strategy.get_feature_importance()

        # Should return a dictionary
        assert isinstance(importance, dict)

        # Keys should be feature names if available
        if hasattr(strategy, 'feature_columns'):
            # If trained, should have importance for features
            if strategy.is_trained:
                assert len(importance) > 0
            else:
                # If not trained, might be empty or have defaults
                pass


# Integration tests
class Test{strategy_class}Integration:
    """Integration tests for {strategy_name}."""

    def test_full_workflow(self, mock_training_data):
        """Test complete train-predict workflow."""
        strategy = {strategy_class}()

        # Train
        strategy.train(mock_training_data)
        assert strategy.is_trained

        # Predict on a sample market
        sample_market = {{
            'id': 'test_market',
            'question': 'Will this test pass?',
            'category': 'tech',
            'volume': 100000,
            'outcome_prices': ['0.6', '0.4'],
            'liquidity': 10000
        }}

        result = strategy.predict(sample_market)

        # Validate result
        assert result.market_id == 'test_market'
        assert result.recommended_bet in ['YES', 'NO', 'PASS']
        assert len(result.reasoning) > 0

    def test_market_data_integration(self, sample_markets):
        """Test with real market data."""
        if not sample_markets:
            pytest.skip("No real market data available")

        strategy = {strategy_class}()

        # Test prediction on real markets
        for market in sample_markets[:3]:  # Test first 3
            result = strategy.predict(market)

            # Should handle real market data
            assert result.market_id == market['id']
            assert result.market_question == market['question']
            assert result.recommended_bet in ['YES', 'NO', 'PASS']

    def test_backtesting_interface(self):
        """Test backtesting capabilities."""
        strategy = {strategy_class}()

        # Should have backtesting interface
        assert hasattr(strategy, 'evaluate_performance')

        # Test with empty predictions
        empty_performance = strategy.evaluate_performance([])
        assert 'error' in empty_performance or 'total_predictions' in empty_performance


if __name__ == "__main__":
    # Run basic tests
    print(f"ðŸ§ª Running basic tests for {strategy_name}...")

    strategy = {strategy_class}()
    print(f"âœ… Strategy initialized: {{strategy.name}}")

    # Test feature preparation
    test_market = {{
        'id': 'test_123',
        'question': 'Will this work?',
        'category': 'tech',
        'volume': 100000,
        'outcome_prices': ['0.55', '0.45']
    }}

    features = strategy.prepare_features(test_market)
    print(f"âœ… Feature preparation: {{features.shape[1]}} features generated")

    print(f"ðŸŽ‰ Basic tests passed for {{strategy_name}}!")
'''

        return test_content

    def generate_strategy_comparison_test(self, strategy_names: List[str]) -> str:
        """Generate a test that compares multiple strategies."""

        strategy_imports = "\n".join(
            [
                f"from polymarket_agents.ml_strategies.{name.lower()} import {name}"
                for name in strategy_names
            ]
        )

        test_content = f'''"""
ML Strategy Comparison Tests

Compares multiple betting strategies against each other.
Generated by GitHub ML Agent on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

import pytest
import numpy as np
import pandas as pd
from typing import Dict, List, Any

{strategy_imports}
from market_analysis_workflow import MarketAnalyzer


class TestStrategyComparison:
    """Compare multiple ML betting strategies."""

    @pytest.fixture
    def strategies(self):
        """Create all strategies for comparison."""
        return {{
            {",\\n            ".join([f'"{name}": {name}()' for name in strategy_names])}
        }}

    @pytest.fixture
    def test_markets(self):
        """Get diverse test markets."""
        analyzer = MarketAnalyzer()
        markets = analyzer.get_high_volume_markets(limit=20)

        # Ensure diversity
        diverse_markets = []
        categories_seen = set()

        for market in markets:
            category = market.get('category', 'unknown')
            if category not in categories_seen and len(categories_seen) < 4:
                diverse_markets.append(market)
                categories_seen.add(category)

        return diverse_markets if diverse_markets else markets[:5]

    @pytest.fixture
    def mock_training_data(self):
        """Create training data for strategy comparison."""
        np.random.seed(42)
        n_samples = 200

        data = []
        for i in range(n_samples):
            volume = np.random.exponential(500000) + 10000
            yes_price = np.random.beta(2, 2)
            category = np.random.choice(['politics', 'sports', 'crypto', 'geopolitics'])

            # Simulate actual outcomes with some market efficiency
            true_prob = yes_price + np.random.normal(0, 0.05)
            true_prob = np.clip(true_prob, 0.01, 0.99)
            actual_outcome = 1 if np.random.random() < true_prob else 0

            data.append({{
                'market_id': f'market_{i}',
                'question': f'Sample market question {i}',
                'category': category,
                'volume': volume,
                'outcome_prices': [str(yes_price), str(1-yes_price)],
                'actual_outcome': actual_outcome
            }})

        return pd.DataFrame(data)

    def test_all_strategies_initialize(self, strategies):
        """Test all strategies can be initialized."""
        for name, strategy in strategies.items():
            assert strategy.name == f"{{name.replace('Predictor', '_')}}{{name}}"
            assert hasattr(strategy, 'train')
            assert hasattr(strategy, 'predict')
            assert hasattr(strategy, 'evaluate_performance')

    def test_strategies_handle_same_market(self, strategies, test_markets):
        """Test all strategies can process the same market."""
        if not test_markets:
            pytest.skip("No test markets available")

        market = test_markets[0]
        results = {{}}

        for name, strategy in strategies.items():
            result = strategy.predict(market)
            results[name] = result

            # Basic validation
            assert result.recommended_bet in ['YES', 'NO', 'PASS']
            assert 0 <= result.confidence <= 1

        # Check that strategies can make different recommendations
        recommendations = [r.recommended_bet for r in results.values()]
        # Allow for some agreement, but not complete uniformity
        assert len(set(recommendations)) >= 1  # At least one unique recommendation

    def test_training_comparison(self, strategies, mock_training_data):
        """Compare training performance across strategies."""
        training_results = {{}}

        for name, strategy in strategies.items():
            # Train strategy
            strategy.train(mock_training_data)

            # Get basic performance info
            training_results[name] = {{
                'trained': strategy.is_trained,
                'feature_count': len(strategy.feature_columns) if hasattr(strategy, 'feature_columns') else 0
            }}

        # All should be trained
        assert all(results['trained'] for results in training_results.values())

        # Should have reasonable feature counts
        for name, results in training_results.items():
            assert results['feature_count'] > 5, f"{{name}} has too few features: {{results['feature_count']}}"

    def test_performance_metrics_comparison(self, strategies, mock_training_data, test_markets):
        """Compare performance metrics across strategies."""
        if not test_markets:
            pytest.skip("No test markets available")

        performance_results = {{}}

        # Train all strategies
        for name, strategy in strategies.items():
            strategy.train(mock_training_data)

        # Test on markets
        for market in test_markets[:5]:  # Test subset for speed
            for name, strategy in strategies.items():
                result = strategy.predict(market)

                if name not in performance_results:
                    performance_results[name] = []

                performance_results[name].append({{
                    'edge': result.edge,
                    'confidence': result.confidence,
                    'position_size': result.position_size,
                    'recommended_bet': result.recommended_bet
                }})

        # Analyze results
        summary = {{}}
        for name, results in performance_results.items():
            edges = [r['edge'] for r in results]
            confidences = [r['confidence'] for r in results]

            summary[name] = {{
                'avg_edge': np.mean(edges),
                'avg_confidence': np.mean(confidences),
                'total_opportunities': sum(1 for r in results if r['recommended_bet'] != 'PASS'),
                'total_recommendations': len(results)
            }}

        # All strategies should have processed markets
        assert all(len(results) > 0 for results in performance_results.values())

        # Print summary for manual inspection
        print("\\nðŸ“Š Strategy Performance Summary:")
        for name, metrics in summary.items():
            print(f"  {{name}}:")
            print(f"    Avg Edge: {{metrics['avg_edge']:.3f}}")
            print(f"    Avg Confidence: {{metrics['avg_confidence']:.3f}}")
            print(f"    Opportunities: {{metrics['total_opportunities']}}/{{metrics['total_recommendations']}}")

    def test_strategy_diversity(self, strategies, test_markets):
        """Test that strategies provide diverse recommendations."""
        if not test_markets:
            pytest.skip("No test markets available")

        diversity_results = []

        for market in test_markets[:10]:  # Test diversity on subset
            recommendations = []

            for name, strategy in strategies.items():
                result = strategy.predict(market)
                recommendations.append(result.recommended_bet)

            # Count unique recommendations
            unique_recs = len(set(recommendations))
            diversity_results.append(unique_recs)

        avg_diversity = np.mean(diversity_results)

        # Should have some diversity (not all strategies agreeing perfectly)
        assert avg_diversity >= 1, "Strategies show no diversity in recommendations"

        print(f"\\nðŸŽ­ Strategy Diversity: {{avg_diversity:.2f}} average unique recommendations per market")

    def test_error_handling(self, strategies):
        """Test strategies handle edge cases gracefully."""
        edge_cases = [
            # Empty market data
            {{}},
            # Missing critical fields
            {{'id': 'test', 'question': 'Test?'}},
            # Extreme values
            {{
                'id': 'extreme',
                'question': 'Extreme case?',
                'category': 'test',
                'volume': 999999999,
                'outcome_prices': ['0.01', '0.99']
            }}
        ]

        for market_data in edge_cases:
            for name, strategy in strategies.items():
                # Should not crash
                result = strategy.predict(market_data)

                # Should return valid result structure
                assert hasattr(result, 'recommended_bet')
                assert result.recommended_bet in ['YES', 'NO', 'PASS', None]


if __name__ == "__main__":
    print("ðŸ§ª Running strategy comparison tests...")

    # Basic initialization test
    strategies = {{
        {",\\n        ".join([f'"{name}": {name}()' for name in strategy_names])}
    }}

    print(f"âœ… Initialized {len(strategies)} strategies:")
    for name in strategies.keys():
        print(f"   â€¢ {{name}}")

    print("ðŸŽ‰ Strategy comparison tests ready!")
'''

        return test_content

    def generate_model_validation_test(self, strategy_name: str) -> str:
        """Generate statistical validation tests for ML models."""

        test_content = f'''"""
ML Model Validation Tests: {strategy_name}

Statistical validation and robustness tests for {strategy_name}.
Generated by GitHub ML Agent on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

import pytest
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.model_selection import cross_val_score, KFold
from typing import Dict, List, Any

from polymarket_agents.ml_strategies.{strategy_name.lower()} import {strategy_name}


class Test{strategy_name}Validation:
    """Statistical validation tests for {strategy_name}."""

    @pytest.fixture
    def strategy(self):
        """Create strategy instance."""
        return {strategy_name}()

    @pytest.fixture
    def large_training_dataset(self):
        """Create large dataset for robust validation."""
        np.random.seed(42)
        n_samples = 500

        data = []
        for i in range(n_samples):
            # Generate more realistic market data
            volume = np.random.exponential(300000) + 50000
            yes_price = np.random.beta(2, 2)

            # Add category-specific patterns
            category = np.random.choice(['politics', 'sports', 'crypto', 'geopolitics'])
            if category == 'politics':
                yes_price = np.random.beta(1.5, 2.5)  # More extreme for politics
            elif category == 'sports':
                yes_price = np.random.beta(2.5, 2.5)  # More centered for sports

            # Simulate market efficiency with realistic noise
            efficiency_factor = np.random.uniform(0.7, 1.3)  # Some markets more efficient
            true_prob = yes_price + np.random.normal(0, 0.08) / efficiency_factor
            true_prob = np.clip(true_prob, 0.02, 0.98)

            actual_outcome = 1 if np.random.random() < true_prob else 0

            data.append({{
                'market_id': f'market_{i}',
                'question': f'Question {i} in {category}',
                'category': category,
                'volume': volume,
                'outcome_prices': [str(yes_price), str(1-yes_price)],
                'liquidity': volume * 0.05,  # 5% liquidity ratio
                'actual_outcome': actual_outcome
            }})

        return pd.DataFrame(data)

    def test_cross_validation_stability(self, strategy, large_training_dataset):
        """Test model stability using cross-validation."""
        # Prepare data
        X = []
        y = []

        for _, row in large_training_dataset.iterrows():
            features = strategy.prepare_features(row.to_dict())
            X.append(features.flatten())
            y.append(row['actual_outcome'])

        X = np.array(X)
        y = np.array(y)

        # Perform cross-validation
        cv = KFold(n_splits=5, shuffle=True, random_state=42)

        # Use a simpler model for CV (RandomForest if available)
        if hasattr(strategy, 'regressor'):
            # For strategies with RandomForest
            cv_scores = cross_val_score(
                strategy.regressor, X, y,
                cv=cv, scoring='neg_mean_squared_error'
            )
            mean_score = -cv_scores.mean()  # Convert back to positive MSE
            std_score = cv_scores.std()

            print(f"\\nðŸ“Š Cross-validation MSE: {{mean_score:.4f}} Â± {{std_score:.4f}}")

            # Model should have reasonable stability
            assert std_score < mean_score, "Model is too unstable across folds"

        else:
            # For strategies without sklearn models
            pytest.skip("Cross-validation not applicable to this strategy type")

    def test_prediction_calibration(self, strategy, large_training_dataset):
        """Test if predicted probabilities are well-calibrated."""
        strategy.train(large_training_dataset)

        # Make predictions on training data
        predictions = []
        actuals = []

        for _, row in large_training_dataset.iterrows():
            result = strategy.predict(row.to_dict())
            predictions.append(result.predicted_probability)
            actuals.append(row['actual_outcome'])

        predictions = np.array(predictions)
        actuals = np.array(actuals)

        # Test calibration using bins
        bins = np.linspace(0, 1, 11)
        bin_indices = np.digitize(predictions, bins) - 1

        calibration_errors = []
        for i in range(len(bins) - 1):
            mask = bin_indices == i
            if np.sum(mask) > 10:  # Need enough samples
                bin_predictions = predictions[mask]
                bin_actuals = actuals[mask]

                predicted_prob = np.mean(bin_predictions)
                actual_prob = np.mean(bin_actuals)

                calibration_error = abs(predicted_prob - actual_prob)
                calibration_errors.append(calibration_error)

        avg_calibration_error = np.mean(calibration_errors) if calibration_errors else 0

        print(f"\\nðŸŽ¯ Calibration Error: {{avg_calibration_error:.4f}}")

        # Well-calibrated model should have low error
        assert avg_calibration_error < 0.15, f"Poor calibration: {{avg_calibration_error:.4f}}"

    def test_feature_importance_stability(self, strategy, large_training_dataset):
        """Test that feature importance is stable across training runs."""
        if not hasattr(strategy, 'get_feature_importance'):
            pytest.skip("Strategy doesn't support feature importance")

        importance_runs = []

        # Run training multiple times with different seeds
        for seed in [42, 123, 456]:
            np.random.seed(seed)
            strategy_copy = {strategy_name}()  # Fresh instance
            strategy_copy.train(large_training_dataset)
            importance = strategy_copy.get_feature_importance()
            importance_runs.append(importance)

        # Check stability of top features
        if importance_runs and all(importance_runs):
            # Get top 3 features from first run
            first_run = importance_runs[0]
            top_features = sorted(first_run.keys(), key=lambda x: first_run[x], reverse=True)[:3]

            # Check if they remain in top 5 in other runs
            for run in importance_runs[1:]:
                run_top_features = set(sorted(run.keys(), key=lambda x: run[x], reverse=True)[:5])
                overlap = len(set(top_features) & run_top_features)
                assert overlap >= 2, f"Feature importance unstable: only {{overlap}}/3 top features consistent"

        print("\\nðŸ”„ Feature importance stability: âœ…")

    def test_market_category_performance(self, strategy, large_training_dataset):
        """Test performance across different market categories."""
        strategy.train(large_training_dataset)

        category_performance = {{}}

        for category in ['politics', 'sports', 'crypto', 'geopolitics']:
            category_data = large_training_dataset[large_training_dataset['category'] == category]

            if len(category_data) < 10:
                continue

            predictions = []
            actuals = []

            for _, row in category_data.iterrows():
                result = strategy.predict(row.to_dict())
                predictions.append(result.predicted_probability)
                actuals.append(row['actual_outcome'])

            if predictions:
                mse = np.mean((np.array(predictions) - np.array(actuals))**2)
                category_performance[category] = {{
                    'mse': mse,
                    'sample_size': len(predictions)
                }}

        # Should perform reasonably on major categories
        assert len(category_performance) >= 2, "Not enough category data for testing"

        avg_mse = np.mean([perf['mse'] for perf in category_performance.values()])
        print(f"\\nðŸ“ˆ Category-wise MSE: {{avg_mse:.4f}}")

        # Model should perform reasonably (MSE < 0.3 is decent for binary prediction)
        assert avg_mse < 0.35, f"Poor category performance: {{avg_mse:.4f}}"

    def test_robustness_to_noise(self, strategy, large_training_dataset):
        """Test model robustness to noisy market data."""
        strategy.train(large_training_dataset)

        # Test on clean data
        clean_predictions = []
        for _, row in large_training_dataset.sample(50, random_state=42).iterrows():
            result = strategy.predict(row.to_dict())
            clean_predictions.append(result.predicted_probability)

        # Test on noisy data (add random noise to prices)
        noisy_predictions = []
        for _, row in large_training_dataset.sample(50, random_state=123).iterrows():
            noisy_row = row.copy()
            # Add noise to prices
            prices = eval(noisy_row['outcome_prices']) if isinstance(noisy_row['outcome_prices'], str) else noisy_row['outcome_prices']
            noisy_prices = [str(min(0.99, max(0.01, float(p) + np.random.normal(0, 0.05)))) for p in prices]
            noisy_row['outcome_prices'] = noisy_prices

            result = strategy.predict(noisy_row.to_dict())
            noisy_predictions.append(result.predicted_probability)

        # Calculate prediction stability
        clean_std = np.std(clean_predictions)
        noisy_std = np.std(noisy_predictions)

        # Predictions shouldn't become dramatically more variable with noise
        stability_ratio = noisy_std / (clean_std + 0.01)  # Avoid division by zero

        print(f"\\nðŸ›¡ï¸ Prediction Stability: {{stability_ratio:.2f}} (lower is better)")

        assert stability_ratio < 2.0, f"Model too sensitive to noise: {{stability_ratio:.2f}}"

    def test_confidence_calibration(self, strategy, large_training_dataset):
        """Test that confidence scores are well-calibrated."""
        strategy.train(large_training_dataset)

        confidence_levels = []
        accuracies = []

        # Bin predictions by confidence
        for confidence_threshold in [0.1, 0.3, 0.5, 0.7, 0.9]:
            correct_predictions = 0
            total_predictions = 0

            for _, row in large_training_dataset.iterrows():
                result = strategy.predict(row.to_dict())

                if result.confidence >= confidence_threshold:
                    total_predictions += 1
                    # Simplified accuracy check (in practice, need actual outcomes)
                    predicted_outcome = 1 if result.predicted_probability > 0.5 else 0
                    actual_outcome = row['actual_outcome']

                    if predicted_outcome == actual_outcome:
                        correct_predictions += 1

            if total_predictions > 10:
                accuracy = correct_predictions / total_predictions
                confidence_levels.append(confidence_threshold)
                accuracies.append(accuracy)

                print(f"   Confidence â‰¥{{confidence_threshold:.1f}}: {{accuracy:.2f}} accuracy ({{total_predictions}} predictions)")

        # Higher confidence should generally correlate with higher accuracy
        if len(accuracies) >= 3:
            # Check if accuracy generally increases with confidence
            accuracy_trend = np.polyfit(confidence_levels, accuracies, 1)[0]
            assert accuracy_trend >= -0.5, f"Confidence not well-calibrated: trend = {{accuracy_trend:.3f}}"


if __name__ == "__main__":
    print(f"ðŸ§ª Running validation tests for {strategy_name}...")

    strategy = {strategy_name}()
    print(f"âœ… Strategy initialized: {{strategy.name}}")

    # Test feature engineering
    test_market = {{
        'id': 'validation_test',
        'question': 'Will validation pass?',
        'category': 'tech',
        'volume': 250000,
        'outcome_prices': ['0.52', '0.48']
    }}

    features = strategy.prepare_features(test_market)
    print(f"âœ… Feature engineering: {{features.shape[1]}} features prepared")

    print(f"ðŸŽ‰ Validation tests ready for {{strategy_name}}!")
'''

        return test_content


def create_ml_test_generator_subagent():
    """
    Create the GitHub ML Test Generator subagent.

    This subagent specializes in:
    - Generating ML-based betting strategy tests
    - Creating pytest files for new strategies
    - Running automated test generation
    - Committing generated tests to GitHub
    """

    return {
        "name": "github-ml-agent",
        "description": "Specializes in generating machine learning tests for betting strategies, creating pytest files, and committing them to GitHub for automated testing and validation.",
        "system_prompt": """You are a specialized ML test generation agent for betting strategies.

Your expertise includes:
- Creating comprehensive ML model tests
- Generating pytest files for new strategies
- Statistical validation of ML models
- Automated test generation and GitHub integration

TEST GENERATION PROCESS:
1. Analyze the ML strategy requirements
2. Generate comprehensive pytest files
3. Create statistical validation tests
4. Commit tests to GitHub repository

OUTPUT FORMAT:
Return your response in this exact structure:

TEST GENERATION SUMMARY
- Strategy: [strategy name and type]
- Tests Created: [number and types of test files]
- Coverage: [test scenarios covered]

FILES GENERATED
- [List of files created with descriptions]
- [File paths and purposes]

VALIDATION APPROACH
- [Statistical tests included]
- [Edge cases covered]
- [Performance metrics validated]

GITHUB INTEGRATION
- [Files committed to repository]
- [Branch and PR information]
- [CI/CD integration notes]

   Keep response under 500 words. Focus on test quality and automation.""",
        "tools": [
            check_github_repo_status,
            search_github_issues,
            create_github_issue,
            create_market_analysis_report,
        ],
        # Use a model good at code generation and testing
        "model": "gpt-4o",  # Could be specialized for code generation
    }


def generate_ml_strategy_test(
    strategy_name: str, strategy_type: str = "predictor", description: str = ""
) -> str:
    """
    Generate a complete ML strategy test file.

    Args:
        strategy_name: Name of the strategy class
        strategy_type: Type of strategy (predictor, detector, etc.)
        description: Description of the strategy

    Returns:
        Complete pytest file content
    """
    generator = MLTestGenerator()

    if strategy_type == "predictor":
        return generator.generate_strategy_test(
            strategy_name, strategy_name, description
        )
    elif strategy_type == "comparison":
        return generator.generate_strategy_comparison_test([strategy_name])
    elif strategy_type == "validation":
        return generator.generate_model_validation_test(strategy_name)
    else:
        return generator.generate_strategy_test(
            strategy_name, strategy_name, description
        )


def commit_ml_tests_to_github(
    test_files: Dict[str, str], commit_message: str = None
) -> Dict[str, Any]:
    """
    Commit generated ML tests to GitHub.

    Args:
        test_files: Dict of filename -> content
        commit_message: Commit message

    Returns:
        Result of the GitHub operations
    """
    if not commit_message:
        commit_message = (
            f"ðŸ¤– ML Tests: Add automated tests for {len(test_files)} strategies"
        )

    results = {"files_created": [], "commit_status": "pending", "errors": []}

    # This would use the GitHub toolkit to create files and commits
    # For now, we'll just simulate and provide the structure

    for filename, content in test_files.items():
        # In real implementation, this would create files via GitHub API
        simulated_result = {
            "filename": filename,
            "size": len(content),
            "path": f"tests/ml_strategies/{filename}",
            "status": "created",
        }
        results["files_created"].append(simulated_result)

    results["commit_message"] = commit_message
    results["branch"] = "main"  # Or feature branch
    results["total_files"] = len(test_files)

    return results
